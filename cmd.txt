docker run --name mps_1body -it --gpus all -d -v ~/HUST_Test:/workspace/HUST_Test -v /tmp/nvidia-mps:/tmp/nvidia-mps --ipc container:mps-daemon pytorch/pytorch:1.11.0-cuda11.3-cudnn8-devel /bin/bash

docker run --name mps-daemon --runtime nvidia -it --gpus all -d -v /tmp/nvidia-mps:/tmp/nvidia-mps -v ~/HUST_Test:/workspace/HUST_Test pytorch/pytorch:1.11.0-cuda11.3-cudnn8-devel /bin/bash

docker run --name pytorch -d -it --gpus all -v ~/HUST_Test:/workspace/HUST_Test pytorch/pytorch:1.11.0-cuda11.3-cudnn8-devel /bin/bash

docker run --name tvm-gpu -it -d --gpus all -v ~/HUST_Test:/workspace/HUST_Test tlcpack/ci-gpu:20220908-060034-62bdc91b1 /bin/bash

docker pull tlcpack/ci-gpu:20220908-060034-62bdc91b1

export TVM_HOME=/tvm
export PYTHONPATH=$TVM_HOME/python:${PYTHONPATH}

ssh wuhao@222.20.94.68 -p 50011

docker exec -ti mps-daemon /bin/bash

export CUDA_VISIBLE_DEVICES=0

git clone git@github.com:ServerlessMLGroup/HUST_Test.git

docker cp ~/HUST_Test pytorch:/workspace

python djx/batch-mps.py --batch_size=1 --gpu_no=0

ps -ef | grep mps

sudo nvidia-smi -i 0 -c EXCLUSIVE_PROCESS

// 容器内执行
echo quit | nvidia-cuda-mps-control

// 容器内执行
nvidia-cuda-mps-control -d

// 容器外执行
echo set_default_active_thread_percentage <mps_percentage> | nvidia-cuda-mps-control

// 容器内执行
python batch-mps.py --batch_size=8
